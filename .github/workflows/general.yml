name: Rust CI/CD backend

on:
  push:
    branches:
      - main
  pull_request:
    types: [opened, synchronize, reopened]
    branches:
      - main

jobs:
    test:
        name: Test with Database and LLM
        runs-on: ubuntu-latest
        timeout-minutes: 30
        
        # Service containers to run alongside the `test` container job
        services:
          postgres:
            image: postgres:14
            env:
              POSTGRES_USER: ${{secrets.POSTGRES_USER}}
              POSTGRES_PASSWORD: ${{secrets.POSTGRES_PASSWORD}}
              POSTGRES_DB: evolveme_db
            ports:
              - 5432:5432
          
          redis:
            image: bitnami/redis:latest
            env:
              REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
            ports:
              - 6379:6379
        
        env:
          POSTGRES__DATABASE__USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES__DATABASE__PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          APP__APPLICATION__USER: ${{ secrets.APP_USER }}
          APP__APPLICATION__PASSWORD: ${{ secrets.APP_PASSWORD }}
          APP_ENVIRONMENT: local
          REDIS__REDIS__PASSWORD: ${{ secrets.REDIS_PASSWORD }}
          LLM__LLM__SERVICE_URL: http://localhost:11434
          LLM__LLM__MODEL_NAME: llama3.2:1b-instruct-q4_K_M  # Use smaller model for CI
          DATABASE_URL: postgres://${{ secrets.POSTGRES_USER }}:${{ secrets.POSTGRES_PASSWORD }}@localhost:5432/evolveme_db

        steps:
            - name: Check out repository code
              uses: actions/checkout@v4
    
            - name: Install the Rust toolchain
              uses: actions-rust-lang/setup-rust-toolchain@v1

            - name: Install system dependencies
              run: |
                sudo apt-get update
                sudo apt-get install -y postgresql-client redis-tools curl

            - name: Install sqlx-cli
              run: |
                if ! command -v sqlx &> /dev/null; then
                  echo "Installing sqlx-cli..."
                  cargo install sqlx-cli \
                    --version=${{ vars.SQLX_CLI_VERSION || '0.8.3' }} \
                    --features ${{ vars.SQLX_CLI_FEATURES || 'postgres' }} \
                    --no-default-features \
                    --locked
                else
                  echo "sqlx-cli already installed"
                fi
                sqlx --version

            - name: Setup and start Ollama
              run: |
                echo "Installing Ollama..."
                curl -fsSL https://ollama.ai/install.sh | sh
                
                echo "Starting Ollama service in background..."
                ollama serve &
                OLLAMA_PID=$!
                echo "OLLAMA_PID=$OLLAMA_PID" >> $GITHUB_ENV
                
                # Give Ollama time to start
                sleep 10
                
                echo "Waiting for Ollama to be ready..."
                for i in {1..30}; do
                  if curl -s http://localhost:11434/api/version >/dev/null 2>&1; then
                    echo "✅ Ollama is ready!"
                    break
                  fi
                  echo "Waiting for Ollama... (attempt $i/30)"
                  sleep 5
                done
                
                # Verify Ollama is responding
                if ! curl -s http://localhost:11434/api/version; then
                  echo "❌ ERROR: Ollama failed to start properly"
                  echo "Checking Ollama process..."
                  ps aux | grep ollama || echo "No ollama process found"
                  echo "Checking Ollama logs..."
                  tail -n 20 ~/.ollama/logs/server.log 2>/dev/null || echo "No logs found"
                  exit 1
                fi

            - name: Pull and test LLM model
              run: |
                echo "Pulling model: $LLM__LLM__MODEL_NAME"
                if timeout 300 ollama pull $LLM__LLM__MODEL_NAME; then
                  echo "✅ Model pulled successfully"
                  
                  echo "Testing model..."
                  if response=$(timeout 60 ollama run $LLM__LLM__MODEL_NAME "Say 'OK' if you understand" 2>/dev/null); then
                    echo "Model response: $response"
                    # Check for various affirmative responses
                    if [[ "$response" =~ [Oo][Kk] ]] || \
                       [[ "$response" =~ [Yy]es ]] || \
                       [[ "$response" =~ [Uu]nderstand ]] || \
                       [[ "$response" =~ [Gg]o\s[a]head ]] || \
                       [[ "$response" =~ [Ii]\s(think\s)?[Dd]o ]]; then
                      echo "✅ Model is working correctly"
                      echo "LLM_AVAILABLE=true" >> $GITHUB_ENV
                    else
                      echo "⚠️ Model test gave unexpected response"
                      echo "LLM_AVAILABLE=false" >> $GITHUB_ENV
                      exit 1
                    fi
                  else
                    echo "⚠️ Model test timed out or failed"
                    echo "LLM_AVAILABLE=false" >> $GITHUB_ENV
                    exit 1
                  fi
                else
                  echo "❌ Model pull failed or timed out"
                  echo "LLM_AVAILABLE=false" >> $GITHUB_ENV
                  exit 1
                fi

            - name: Verify LLM availability
              run: |
                if [ "$LLM_AVAILABLE" != "true" ]; then
                  echo "❌ LLM is not available. Aborting workflow."
                  exit 1
                fi

            - name: Setup database and services
              run: |                
                # Run the CI initialization
                ./scripts/init_db_and_redis_ci.sh

            - name: Generate SQLx prepare files
              env:
                RUSTFLAGS: "-A warnings"
                SQLX_OFFLINE: false
              run: |
                cargo sqlx prepare --workspace -- --all-targets

            - name: Run tests with LLM
              env:
                RUSTFLAGS: "-A warnings"
                SQLX_OFFLINE: false
              run: |
                echo "Running tests..."
                if [ "$LLM_AVAILABLE" = "true" ]; then
                  echo "Running tests with LLM enabled"
                  cargo test --all -- --test-threads=1
                else
                  echo "Running tests with LLM"
                  cargo test --all -- --test-threads=1
                fi
                
            - name: Check SQLx queries are fresh
              env:
                RUSTFLAGS: "-A warnings"
              run: |
                cargo sqlx prepare --workspace --check -- --all-targets

            - name: Cleanup
              if: always()
              run: |
                if [ -n "$OLLAMA_PID" ]; then
                  echo "Stopping Ollama (PID: $OLLAMA_PID)"
                  kill $OLLAMA_PID || true
                fi

    build_and_push:
      name: Build and Push Multi-Architecture Docker image
      needs: [test]
      runs-on: ubuntu-latest
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      
      steps:
        - name: Check out repository code
          uses: actions/checkout@v4
          
        - name: Set up QEMU for multi-architecture builds
          uses: docker/setup-qemu-action@v3
          
        - name: Set up Docker Buildx
          uses: docker/setup-buildx-action@v3
          
        - name: Login to GitHub Container Registry
          uses: docker/login-action@v3
          with:
            registry: ghcr.io
            username: ${{ github.actor }}
            password: ${{ secrets.GITHUB_TOKEN }}
            
        - name: Extract metadata for Docker
          id: meta
          uses: docker/metadata-action@v5
          with:
            images: ghcr.io/${{ github.repository }}
            tags: |
              type=sha,format=long
              type=ref,event=branch
              latest
              
        - name: Build and push multi-architecture Docker image
          uses: docker/build-push-action@v5
          with:
            context: .
            platforms: linux/amd64,linux/arm64
            push: true
            tags: ${{ steps.meta.outputs.tags }}
            labels: ${{ steps.meta.outputs.labels }}
            cache-from: type=gha
            cache-to: type=gha,mode=max